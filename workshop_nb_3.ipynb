{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66200b7d-fe86-4ce4-994b-6a6fdde11b30",
   "metadata": {},
   "source": [
    "# Workshop Notebook 3 - RAG\n",
    "\n",
    "In this notebook, we will ingest the entire 92-document dataset into Aryn and implement RAG to answer some questions over them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ffbe6a-928a-45db-a2b8-b6ad160fbf00",
   "metadata": {},
   "source": [
    "## Full ingestion script\n",
    "\n",
    "In the previous notebook we walked through how you might come up with a sycamore ingestion script\n",
    "to write a bunch of documents to Aryn (or another database). Now let's scale that. Because this is\n",
    "a new notebook, I've gone and copied the ingestion script from the previous notebook, and also\n",
    "condensed it a little. I've also partitioned the documents ahead of time, as this can take a little \n",
    "while (our DOS protection is to limit the concurrent requests in an account) - so this script will \n",
    "get them from a materialize. This is one of the things you downloaded with `make downloads`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28b796-defa-4af0-a513-0e639a74104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import sycamore\n",
    "from sycamore import MaterializeSourceMode\n",
    "from sycamore.data import Element\n",
    "from sycamore.llms.openai import OpenAI, OpenAIModels\n",
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "from sycamore.transforms.extract_schema import LLMPropertyExtractor\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "from sycamore.transforms.embed import OpenAIEmbedder\n",
    "from aryn_sdk.client.client import Client\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "pdf_dir = repo_root / \"files\" / \"earnings_calls\"\n",
    "materialize_dir = repo_root / \"materialize\"\n",
    "\n",
    "gpt4o = OpenAI(OpenAIModels.GPT_4O)\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"quarter\": {\"type\": \"string\", \"description\": \"Quarter of the earnings call, it should be in the format of Q1, Q2, Q3, Q4\"},\n",
    "        \"date\":{\"type\": \"string\", \"description\": \"The date of the earnings call\"},\n",
    "        \"company_name\": {\"type\": \"string\", \"description\": \"The name of the company in the earnings call\"},\n",
    "        \"company_ticker\": {\"type\": \"string\", \"description\": \"The stock ticker of the company in the earnings call\"},\n",
    "    }\n",
    "}\n",
    "\n",
    "def mark_speakers(elt: Element) -> Element:\n",
    "    if not elt.text_representation:\n",
    "        return elt\n",
    "\n",
    "    external_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--[^\\S\\n\\t].*--', elt.text_representation)\n",
    "    internal_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--.*', elt.text_representation)\n",
    "    if elt.text_representation.strip() == 'Operator':\n",
    "        elt.properties['speaker_name'] = 'Operator'\n",
    "        elt.properties['speaker_role'] = 'Operator'\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data['_break'] = True\n",
    "    elif external_speaker:\n",
    "        location = elt.text_representation.find('--')\n",
    "        location2 = location + elt.text_representation[location+2:].find('--')\n",
    "        elt.properties['speaker_name'] = elt.text_representation[:location].strip()\n",
    "        elt.properties['speaker_external_org'] = elt.text_representation[location+2:location2+1].strip()\n",
    "        elt.properties['speaker_role'] = elt.text_representation[location2+4:].strip()\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    elif internal_speaker:\n",
    "        location = elt.text_representation.find('--')\n",
    "        elt.properties['speaker_name'] = elt.text_representation[:location].strip()\n",
    "        elt.properties['speaker_role'] = elt.text_representation[location+2:].strip()\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    return elt\n",
    "\n",
    "aryn_client = Client()\n",
    "aryn_docset = aryn_client.create_docset(name = \"haystack-workshop-all\")\n",
    "docset_id = aryn_docset.value.docset_id\n",
    "\n",
    "ctx = sycamore.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9407a9f-6ed9-4899-8576-5181d38f6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ctx.read.binary(paths = str(pdf_dir), binary_format = \"pdf\")\n",
    "    .partition(ArynPartitioner())\n",
    "    .materialize(path=materialize_dir / \"alldocs-partitioned\", source_mode=MaterializeSourceMode.USE_STORED)\n",
    "    .filter_elements(lambda e: e.type in (\"Section-header\", \"Text\"))\n",
    "    .extract_properties(LLMPropertyExtractor(llm=gpt4o, schema=schema))\n",
    "    .map_elements(mark_speakers)\n",
    "    .merge(MarkedMerger())\n",
    "    .embed(OpenAIEmbedder(model_name=\"text-embedding-3-small\"))\n",
    "    .spread_properties(['path', 'entity'])\n",
    "    .write.aryn(aryn_url=\"https://api.aryn.ai/v1/storage\", docset_id=docset_id, autoschema=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee170b-42da-4e86-a47a-38bfc399b0d5",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Alrighty, now let's answer some questions on the data!\n",
    "\n",
    "We've come up with a list of questions that we think will be interesting to try to answer - some are easy and some are hard:\n",
    "\n",
    "0. In the Broadcom earnings call, what details did the CFO, Kirsten Spears, discuss about the VMware acqusition?\n",
    "1. What was the change in stock price on the day of the Q2 2024 AirBnB earnings call?\n",
    "2. List all the speakers in the MongoDB Q4 2024 earnings call.\n",
    "3. List all the speakers in the Broadcom Q4 2024 earnings call.\n",
    "4. How many customers did MongoDB have at the end of the Q1 2024 quarter?\n",
    "5. What was the first PLTR earnings call where Anduril is mentioned?\n",
    "6. List all the companies that mentioned inflation and give me a count of the number of times each of the companies mentioned inflation.\n",
    "7. Summarize how the VMWare acquisition contributed to revenue changes for Broadcom quarter over quarter.\n",
    "8. Summarize how Intuitâ€™s latest AI powered platform, Intuit Assist is being integrated through its products. Give me a quarter by quarter break down of the progress. \n",
    "9. Summarize all the mergers and acquisitions that happened in 2024 and give a breakdown of how each acquisition impacted earnings.\n",
    "10. Summarize how AI integration is progressing across each company's products. Give me a quarter by quarter break down of the progress per company and overall.\n",
    "\n",
    "With the data ingested in Aryn, we should be able to build programmatically answer these question by retrieving the data and using an LLM. We'll \n",
    "start with RAG (Retrieval Augmented Generation), which can answer most of the simpler questions, but getting further down the list, RAG starts to \n",
    "break down. There's simply too much data that needs to be retrieved to do it all with an LLM call. In the next notebook, we will use sycamore to\n",
    "answer these harder questions, but for now let's knock out the simpler retrieval-based ones.\n",
    "\n",
    "## Simple RAG implementation\n",
    "\n",
    "RAG is essentially comprised of two steps: a query step and a llm step. In other words, you execute a search query, and then you hand an LLM the \n",
    "search results and the original question, and it crunches the search results into an answer. There are about a million variants, but for the\n",
    "purposes of expediency we'll just use basic RAG here. We can query Aryn using aryn_sdk, so let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eec774-0473-4ac0-96ed-45e04523a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.client.client import Client as ArynClient\n",
    "\n",
    "aryn_client = ArynClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb0754-66e2-4690-8eb5-d7ea1b3937dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had way too much fun making this to not include it. \n",
    "# Basically just an extremely fancy way of printing all the DocSets in your account\n",
    "import rich\n",
    "\n",
    "dtable = rich.table.Table(title=\"Docsets\")\n",
    "dtable.add_column(\"docset_id\")\n",
    "dtable.add_column(\"name\")\n",
    "dtable.add_column(\"created_at\")\n",
    "dtable.add_column(\"size\")\n",
    "\n",
    "dses = list(aryn_client.list_docsets())\n",
    "for ds in dses:\n",
    "    dtable.add_row(ds.docset_id, ds.name, ds.created_at.isoformat(), str(ds.size))\n",
    "\n",
    "rich.console.Console().print(dtable)\n",
    "\n",
    "# Write the docset id to a file to pick up in the next notebook:\n",
    "with open(\"docset_id\", \"w\") as f:\n",
    "    f.write(docset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eee26f-846c-47bd-bf3b-84687b6404b2",
   "metadata": {},
   "source": [
    "### Search\n",
    "First let's play around a little with the search API. `aryn_client.search()` accepts two parameters: a `docset_id` and a `search_request`.\n",
    "\n",
    "An Aryn `SearchRequest` has the following parameters:\n",
    "\n",
    "- `query`: the query string\n",
    "- `query_type`: the kind of query to execute. One of \"keyword\", \"lexical\", \"vector\", and \"hybrid\"\n",
    "- `properties_filter`: a filter expression. More on those later\n",
    "- `return_type`: either \"element\" or \"doc\". Whether to return individual elements or whole documents.\n",
    "\n",
    "### Filters\n",
    "Aryn filter syntax is comprised of expressions formatted like so: `\"(property = \\\"value\\\")`, where the\n",
    "property name is given in dotted notation. String values are double-quoted. Other comparison operators (<, >, <=, >=, <>) are supported.\n",
    "Any number of expressions can be joined with ANDs. No grouping is allowed though.\n",
    "\n",
    "Here's an example query. Feel free to mess with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569f1d8-884e-48d5-8960-514b5e580793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.types.search import SearchRequest\n",
    "\n",
    "response = aryn_client.search(\n",
    "    docset_id = docset_id,\n",
    "    query = SearchRequest(\n",
    "        query=\"What is Tesla up to these days?\",\n",
    "        query_type=\"vector\",\n",
    "        return_type=\"doc\",\n",
    "        properties_filter=\"(properties.entity.company_ticker = \\\"TSLA\\\")\"\n",
    "    )\n",
    ")\n",
    "\n",
    "response.value.results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93afb4e-2a2a-4038-9d7f-2fca2e87a946",
   "metadata": {},
   "source": [
    "Now we'll write a relatively simple RAG function to reuse for the first several questions. \n",
    "\n",
    "I'll use the sycamore LLM interface because it's what I'm most familiar with and it's fairly easy to use. A sycamore LLM has a \n",
    "`generate` method that accepts a `RenderedPrompt` which is made up of `RenderedMessage`s, following the messages API that the \n",
    "model providers seem to have settled on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58096189-0f3c-4050-9d35-41137e0b0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.types.search import SearchRequest\n",
    "from sycamore.llms.openai import OpenAI, OpenAIModels\n",
    "from sycamore.llms.prompts.prompts import RenderedPrompt, RenderedMessage\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_4O)\n",
    "\n",
    "def rag(question: str, search_request: SearchRequest) -> str:\n",
    "    search_result = aryn_client.search(docset_id = docset_id, query = search_request)\n",
    "\n",
    "    messages = [RenderedMessage(role=\"user\", content=f\"Using the provided documents, answer the question: {question}\")]\n",
    "    for sr in search_result.value.results:\n",
    "        # We don't want to include the embeddings in the prompt - \n",
    "        # It will just take up space with thousands of random numbers.\n",
    "        sr.pop(\"embedding\", None)\n",
    "        if \"elements\" in sr:\n",
    "            for elt in sr[\"elements\"]:\n",
    "                elt.pop(\"embedding\", None)\n",
    "        # This really isn't super intelligent. We just dump each search result \n",
    "        # out as a string and let the LLM decide what to pay attention to\n",
    "        messages.append(RenderedMessage(role=\"user\", content=str(sr)))\n",
    "        \n",
    "    return llm.generate(prompt=RenderedPrompt(messages=messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ed62b-2110-44f4-926e-4377eda0bb14",
   "metadata": {},
   "source": [
    "### Question 1: What was the change in stock price on the day of the Q2 2024 AirBnB earnings call?\n",
    "\n",
    "Correct answer: 2.92%\n",
    "\n",
    "It turns out that plain, unfiltered BM25 search is sufficient to answer this question. Note that we're asking\n",
    "for elements back rather than documents - typically this is what you want to do in RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e6490-ca89-43b0-befb-f2673b5615dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What was the change in stock price on the day of the Q2 2024 AirBnB earnings call?\"\n",
    "\n",
    "search_request1 = SearchRequest(\n",
    "    query=question1,\n",
    "    query_type=\"lexical\",\n",
    "    return_type=\"element\",\n",
    ")\n",
    "\n",
    "answer1 = rag(question1, search_request1)\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf8b80-a236-4c5d-8aea-b0236913ab1a",
   "metadata": {},
   "source": [
    "### Question 2: List all the speakers in the MongoDB Q4 2024 earnings call.\n",
    "\n",
    "Correct answer: <It's like a list of 12 people>\n",
    "\n",
    "Here we need a filter - specifically we want only documents in Q4 for MongoDB (we only have 2024 data but you'd \n",
    "probably want to add that filter too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6db41b-4fe8-4fbd-8331-e7655c93be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"List all the speakers in the MongoDB Q4 2024 earnings call.\"\n",
    "\n",
    "search_request2 = SearchRequest(\n",
    "    query=question2,\n",
    "    query_type=\"lexical\",\n",
    "    return_type=\"element\",\n",
    "    properties_filter=\"(properties.entity.company_ticker=\\\"MDB\\\") AND (properties.entity.quarter=\\\"Q4\\\")\",\n",
    ")\n",
    "\n",
    "answer2 = rag(question2, search_request2)\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b24fa-0f56-4043-8a6e-8174cf9cf195",
   "metadata": {},
   "source": [
    "### Question 3: List all the speakers in the Broadcom Q4 2024 earnings call.\n",
    "\n",
    "Correct answer: <It's like a list of 16 people>\n",
    "\n",
    "Write the search request yourself! (Note: The stock ticker for Broadcom is the extremely intuitive AVGO. Filtering on company name also works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad7818-1d95-4f1e-b415-b98d4aaed05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = \"List all the speakers in the Broadcom Q4 2024 earnings call.\"\n",
    "\n",
    "search_request3 = ...\n",
    "\n",
    "answer3 = rag(question3, search_request3)\n",
    "print(answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dccc3-70e9-4e5e-b630-8ad3d1ae37bd",
   "metadata": {},
   "source": [
    "### Question 4: How many customers did MongoDB have at the end of the Q1 2024 quarter?\n",
    "\n",
    "Correct answer: >49.2k\n",
    "\n",
    "Write the search request yourself! (I didn't need any filters for this one - just lexical search did the trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c431cf-8fc2-4e60-86a8-8a583718d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question4 = \"How many customers did MongoDB have at the end of the Q1 2024 quarter?\"\n",
    "\n",
    "search_request4 = ...\n",
    "\n",
    "answer4 = rag(question4, search_request4)\n",
    "print(answer4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb0a190-dec6-4ffd-9e41-3bfb1ca95020",
   "metadata": {},
   "source": [
    "### Question 5: What was the first PLTR earnings call where Anduril is mentioned?\n",
    "\n",
    "Correct answer: Q3 2024 (November 4)\n",
    "\n",
    "This question can be answered by RAG, but this is mostly due to the fact that our dataset is rather small. You could imagine that\n",
    "if our data contained hundreds of 2020s Palantir reports, it would be hard to be sure that we were retrieving the first document\n",
    "referencing the acquisition, and therefore difficult to tell if the RAG answer is correct. What we'd actually like to do is get\n",
    "all the records mentioning the acquisition, sort them by date, and then return the first one.\n",
    "\n",
    "For completeness, here's the rag implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3b46b-a3fc-4acd-b381-0a990196ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "question5 = \"What was the first PLTR earnings call where Anduril is mentioned?\"\n",
    "\n",
    "search_request5 = SearchRequest(\n",
    "    query=question5,\n",
    "    query_type=\"lexical\",\n",
    "    return_type=\"element\",\n",
    "    properties_filter=\"(properties.entity.company_ticker=\\\"PLTR\\\")\"\n",
    ")\n",
    "\n",
    "answer5 = rag(question5, search_request5)\n",
    "print(answer5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab08e56-ba2e-4115-9e64-323ac2b02915",
   "metadata": {},
   "source": [
    "## Question 6: List all the companies that mentioned inflation and give me a count of the number of times each of the companies mentioned inflation.\n",
    "\n",
    "Correct answer:\n",
    "\n",
    "- Amazon: 3 \n",
    "- AstraZeneca: 3 \n",
    "- ...\n",
    "- Camden Property Trust: 1\n",
    "\n",
    "I was completely unable to get this question to work, as it requires working over all chunks that mention inflation and counting them\n",
    "per company. If you can get RAG to answer this (especially in a way that scales), kudos. Here's what it would probably look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5e752-aac7-47d2-ade1-8f3d1fef5bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question6 = \"List all the companies that mentioned inflation and give me a count of the number of times each of the companies mentioned inflation.\"\n",
    "\n",
    "search_request6 = SearchRequest(\n",
    "    query=\"inflation\",\n",
    "    query_type=\"vector\",\n",
    "    return_type=\"element\",\n",
    ")\n",
    "\n",
    "answer6 = rag(question6, search_request6)\n",
    "print(answer6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7032e1-c93d-4785-b11e-da3faa98cd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
