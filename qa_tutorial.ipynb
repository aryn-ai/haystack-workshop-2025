{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ee170b-42da-4e86-a47a-38bfc399b0d5",
   "metadata": {},
   "source": [
    "# Question Answering Tutorial\n",
    "\n",
    "In the previous tutorial we ingested a bunch of earnings call transcripts into Aryn with Sycamore. Now let's answer some questions on the data!\n",
    "\n",
    "We've come up with a list of questions that we think will be interesting to try to answer - some are easy and some are hard:\n",
    "\n",
    "0. In the Broadcom earnings call, what details did the CFO, Kirsten Spears, discuss about the VMware acqusition?\n",
    "1. What was the change in stock price on the day of the Q2 2024 AirBnB earnings call?\n",
    "2. List all the speakers in the MongoDB Q4 2024 earnings call.\n",
    "3. List all the speakers in the Broadcom Q4 2024 earnings call.\n",
    "4. How many customers did MongoDB have at the end of the Q1 2024 quarter?\n",
    "5. What was the first PLTR earnings call where Anduril is mentioned?\n",
    "6. Summarize how the VMWare acquisition contributed to revenue changes for Broadcom quarter over quarter.\n",
    "7. Summarize how Intuitâ€™s latest AI powered platform, Intuit Assist is being integrated through its products. Give me a quarter by quarter break down of the progress.\n",
    "8. List all the companies that mentioned inflation and give me a count of the number of times each of the companies mentioned inflation.\n",
    "9. Summarize all the mergers and acquisitions that happened in 2024 and give a breakdown of how each acquisition impacted earnings.\n",
    "10. Summarize how AI integration is progressing across each company's products. Give me a quarter by quarter break down of the progress per company and overall.\n",
    "\n",
    "With the data ingested in Aryn, we should be able to build programmatically answer these question by retrieving the data and using an LLM. We'll \n",
    "start with RAG (Retrieval Augmented Generation), which can answer most of the simpler questions, but getting further down the list, RAG starts to \n",
    "break down. There's simply too much data that needs to be retrieved to do it all with an LLM call. In practice, most RAG practitioners will end up \n",
    "building complicated agentic flowy systems to break down the question into more manageable pieces - here, we'll use sycamore to spellout query plans \n",
    "which can answer these 'sweep and harvest'-style questions.\n",
    "\n",
    "## Simple RAG implementation\n",
    "\n",
    "RAG is essentially comprised of two steps: a query step and a llm step. In other words, you execute a search query, and then you hand an LLM the \n",
    "search results and the original question, and it crunches the search results into an answer. There are about a million variants, but for the\n",
    "purposes of expediency we'll just use basic RAG here. We can query Aryn using aryn_sdk, so let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eec774-0473-4ac0-96ed-45e04523a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.client.client import Client as ArynClient\n",
    "\n",
    "aryn_client = ArynClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb0754-66e2-4690-8eb5-d7ea1b3937dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had way too much fun making this to not include it. \n",
    "# Basically just an extremely fancy way of printing all the DocSets in your account\n",
    "import rich\n",
    "\n",
    "dtable = rich.table.Table(title=\"Docsets\")\n",
    "dtable.add_column(\"docset_id\")\n",
    "dtable.add_column(\"name\")\n",
    "dtable.add_column(\"created_at\")\n",
    "dtable.add_column(\"size\")\n",
    "\n",
    "dses = list(aryn_client.list_docsets())\n",
    "for ds in dses:\n",
    "    dtable.add_row(ds.docset_id, ds.name, ds.created_at.isoformat(), str(ds.size))\n",
    "\n",
    "rich.console.Console().print(dtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48625f-8683-4e35-9571-d39ac3c792fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok ok ok let's get the docset_id of the docset we created at the end of the ingestion tutorial\n",
    "with open(\"docset_id\", \"r\") as f:\n",
    "    docset_id = f.read()\n",
    "\n",
    "print(docset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eee26f-846c-47bd-bf3b-84687b6404b2",
   "metadata": {},
   "source": [
    "### Search\n",
    "First let's play around a little with the search API. `aryn_client.search()` accepts two parameters: a `docset_id` and a `search_request`.\n",
    "\n",
    "An Aryn `SearchRequest` has the following parameters:\n",
    "\n",
    "- `query`: the query string\n",
    "- `query_type`: the kind of query to execute. One of \"keyword\", \"lexical\", \"vector\", and \"hybrid\"\n",
    "- `properties_filter`: a filter expression. More on those later\n",
    "- `return_type`: either \"element\" or \"doc\". Whether to return individual elements or whole documents.\n",
    "\n",
    "### Filters\n",
    "Aryn filter syntax is comprised of expressions formatted like so: `\"(property = \\\"value\\\")`, where the\n",
    "property name is given in dotted notation. String values are double-quoted. Other comparison operators (<, >, <=, >=, <>) are supported.\n",
    "Any number of expressions can be joined with ANDs. No grouping is allowed though.\n",
    "\n",
    "Here's an example query. Feel free to mess with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569f1d8-884e-48d5-8960-514b5e580793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.types.search import SearchRequest\n",
    "\n",
    "aryn_client.search(\n",
    "    docset_id = docset_id,\n",
    "    query = SearchRequest(\n",
    "        query=\"What is Tesla up to these days?\",\n",
    "        query_type=\"vector\",\n",
    "        return_type=\"doc\",\n",
    "        properties_filter=\"(properties.entity.company_ticker = \\\"TSLA\\\")\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93afb4e-2a2a-4038-9d7f-2fca2e87a946",
   "metadata": {},
   "source": [
    "Now we'll write a relatively simple RAG function to reuse for the first several questions. \n",
    "\n",
    "I'll use the sycamore LLM interface because it's what I'm most familiar with and it's fairly easy to use. A sycamore LLM has a \n",
    "`generate` method that accepts a `RenderedPrompt` which is made up of `RenderedMessage`s, following the messages API that the \n",
    "model providers seem to have settled on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58096189-0f3c-4050-9d35-41137e0b0c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.types.search import SearchRequest\n",
    "from sycamore.llms.openai import OpenAI, OpenAIModels\n",
    "from sycamore.llms.prompts.prompts import RenderedPrompt, RenderedMessage\n",
    "\n",
    "llm = OpenAI(OpenAIModels.GPT_4O)\n",
    "\n",
    "def rag(question: str, search_request: SearchRequest) -> str:\n",
    "    search_result = aryn_client.search(docset_id = docset_id, query = search_request)\n",
    "\n",
    "    messages = [RenderedMessage(role=\"user\", content=f\"Using the provided documents, answer the question: {question}\")]\n",
    "    for sr in search_result.value.results:\n",
    "        # We don't want to include the embeddings in the prompt - \n",
    "        # It will just take up space with thousands of random numbers.\n",
    "        sr.pop(\"embedding\", None)\n",
    "        if \"elements\" in sr:\n",
    "            for elt in sr[\"elements\"]:\n",
    "                elt.pop(\"embedding\", None)\n",
    "        # This really isn't super intelligent. We just dump each search result \n",
    "        # out as a string and let the LLM decide what to pay attention to\n",
    "        messages.append(RenderedMessage(role=\"user\", content=str(sr)))\n",
    "        \n",
    "    return llm.generate(prompt=RenderedPrompt(messages=messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ed62b-2110-44f4-926e-4377eda0bb14",
   "metadata": {},
   "source": [
    "### Question 1: What was the change in stock price on the day of the Q2 2024 AirBnB earnings call?\n",
    "\n",
    "Correct answer: 2.92%\n",
    "\n",
    "It turns out that plain, unfiltered BM25 search is sufficient to answer this question. Note that we're asking\n",
    "for elements back rather than documents - typically this is what you want to do in RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e6490-ca89-43b0-befb-f2673b5615dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"What was the change in stock price on the day of the Q2 2024 AirBnB earnings call?\"\n",
    "\n",
    "search_request1 = SearchRequest(\n",
    "    query=question1,\n",
    "    query_type=\"lexical\",\n",
    "    return_type=\"element\",\n",
    ")\n",
    "\n",
    "answer1 = rag(question1, search_request1)\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf8b80-a236-4c5d-8aea-b0236913ab1a",
   "metadata": {},
   "source": [
    "### Question 2: List all the speakers in the MongoDB Q4 2024 earnings call.\n",
    "\n",
    "Correct answer: <It's like a list of 12 people>\n",
    "\n",
    "Here we need a filter - specifically we want only documents in Q4 for MongoDB (we only have 2024 data but you'd \n",
    "probably want to add that filter too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6db41b-4fe8-4fbd-8331-e7655c93be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"List all the speakers in the MongoDB Q4 2024 earnings call.\"\n",
    "\n",
    "search_request2 = SearchRequest(\n",
    "    query=question2,\n",
    "    query_type=\"lexical\",\n",
    "    return_type=\"element\",\n",
    "    properties_filter=\"(properties.entity.company_ticker=\\\"MDB\\\") AND (properties.entity.quarter=\\\"Q4\\\")\",\n",
    ")\n",
    "\n",
    "answer2 = rag(question2, search_request2)\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b24fa-0f56-4043-8a6e-8174cf9cf195",
   "metadata": {},
   "source": [
    "### Question 3: List all the speakers in the Broadcom Q4 2024 earnings call.\n",
    "\n",
    "Correct answer: <It's like a list of 16 people>\n",
    "\n",
    "Write the search request yourself! (Note: The stock ticker for Broadcom is the extremely intuitive AVGO. Filtering on company name also works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad7818-1d95-4f1e-b415-b98d4aaed05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question3 = \"List all the speakers in the Broadcom Q4 2024 earnings call.\"\n",
    "\n",
    "search_request3 = ...\n",
    "\n",
    "answer3 = rag(question3, search_request3)\n",
    "print(answer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2dccc3-70e9-4e5e-b630-8ad3d1ae37bd",
   "metadata": {},
   "source": [
    "### Question 4: How many customers did MongoDB have at the end of the Q1 2024 quarter?\n",
    "\n",
    "Correct answer: >49.2k\n",
    "\n",
    "Write the search request yourself! (I didn't need any filters for this one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c431cf-8fc2-4e60-86a8-8a583718d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question4 = \"How many customers did MongoDB have at the end of the Q1 2024 quarter?\"\n",
    "\n",
    "search_request4 = ...\n",
    "\n",
    "answer4 = rag(question4, search_request4)\n",
    "print(answer4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb0a190-dec6-4ffd-9e41-3bfb1ca95020",
   "metadata": {},
   "source": [
    "### Question 5: What was the first PLTR earnings call where Anduril is mentioned?\n",
    "\n",
    "Correct answer: Q3 2024 (November 4)\n",
    "\n",
    "This question can be answered by RAG, but this is mostly due to the fact that our dataset is rather small. You could imagine that\n",
    "if our data contained hundreds of 2020s Palantir reports, it would be hard to be sure that we were retrieving the first document\n",
    "referencing the acquisition, and therefore difficult to tell if the RAG answer is correct. What we'd actually like to do is get\n",
    "all the records mentioning the acquisition, sort them by date, and then return the first one.\n",
    "\n",
    "For completeness, here's the rag implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3b46b-a3fc-4acd-b381-0a990196ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "question5 = \"What was the first PLTR earnings call where Anduril is mentioned?\"\n",
    "\n",
    "search_request5 = SearchRequest(\n",
    "    query=question5,\n",
    "    query_type=\"lexical\",\n",
    "    return_type=\"element\",\n",
    "    properties_filter=\"(properties.entity.company_ticker=\\\"PLTR\\\")\"\n",
    ")\n",
    "\n",
    "answer5_rag = rag(question5, search_request5)\n",
    "print(answer5_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f63a1-e2c0-4247-acf0-156d2e2082db",
   "metadata": {},
   "source": [
    "Let's use sycamore to answer the question though. We'll start by reading the DocSet from Aryn.\n",
    "\n",
    "Quiz: We'll be reading this docset from Aryn a bunch. What can we do to cache a local copy to speed this up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4ac09-8b1b-4642-8b6e-6baef9d2d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sycamore\n",
    "from sycamore import MaterializeSourceMode\n",
    "from sycamore.data import Element, Document\n",
    "context = sycamore.init()\n",
    "\n",
    "read_me_seymoure = context.read.aryn(docset_id = docset_id, aryn_url=\"https://api.aryn.ai/v1/storage\").materialize(path=\"materialize/temp\", source_mode=MaterializeSourceMode.USE_STORED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a249360-c1d8-469f-b08b-e6da3da0c5e9",
   "metadata": {},
   "source": [
    "The documents come out with an \"_original_elements\" property which contains a copy of the elements. We use this in the UI to render bounding boxes of elements on the \n",
    "documents (circumventing any chunking) but for our purposes this is a load of crap so we'll add a transform to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f666d9-09e7-4d70-9949-be41ce8e777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_original_elements(doc: Document) -> Document:\n",
    "    doc.properties.pop(\"_original_elements\", None)\n",
    "    return doc\n",
    "    \n",
    "cleaned_docset = read_me_seymoure.map(remove_original_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c8881a-abdf-4679-8166-9f720477b38e",
   "metadata": {},
   "source": [
    "Now we'll actually answer the question. First we'll want to filter to only Palantir documents. Next we'll filter out any elements \n",
    "that don't contain the string 'anduril'. We'll apply another filter for documents that still have elements. We'll standardize our \n",
    "date property - since we extracted it with an LLM, it's just an unstructured string. `DateTimeStandardizer` will parse it into a \n",
    "python `DateTime` object, which we can sort by.\n",
    "\n",
    "Once we've done all of that, we can simply take the first document and report, for example, the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f193c94-9ad1-4dd1-8319-9850f3a4d09c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sycamore.transforms import DateTimeStandardizer\n",
    "\n",
    "vwmare_docset_sorted = (\n",
    "    cleaned_docset\n",
    "    .filter(lambda doc: doc.properties['entity']['company_ticker'] == 'PLTR')\n",
    "    .filter_elements(lambda elt: \"anduril\" in (elt.text_representation or \"\").lower())\n",
    "    .filter(lambda doc: len(doc.elements) > 0)\n",
    "    .map(lambda doc: DateTimeStandardizer.standardize(doc, key_path = [\"properties\",\"entity\",\"date\"]))\n",
    "    .sort(descending=False, field=\"properties.entity.dateTime\")\n",
    ")\n",
    "answer5_sycamore = vwmare_docset_sorted.take(1)[0].properties['entity']['day']\n",
    "print(answer5_sycamore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e8ff3-2c87-4f9c-833d-12e7999e47d8",
   "metadata": {},
   "source": [
    "## Question 9: Summarize all the mergers and acquisitions that happened in 2024 and give a breakdown of how each acquisition impacted earnings.\n",
    "\n",
    "Correct answer: A summary of all the mergers and acquisitions. I ain't writing that out.\n",
    "\n",
    "Plan:\n",
    "\n",
    "1. Filter elements by whether \"merger\" or \"acquisition\" is in the text\n",
    "2. Summarize everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9034a8b1-0d7b-46c9-8047-90590c24798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, a filter_elements. This is fairly simple\n",
    "from sycamore.data import Element\n",
    "\n",
    "def filter_mna(elt: Element) -> bool:\n",
    "    return \"acquisition\" in elt.text_representation.lower() or \"merger\" in elt.text_representation.lower() \n",
    "\n",
    "mna_filtered_ds = cleaned_docset.filter_elements(filter_mna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d467364-aabd-473a-898e-414fda51d2a0",
   "metadata": {},
   "source": [
    "## Summarize\n",
    "\n",
    "Sycamore exports a function called `summarize_data` which attempts to summarize an entire DocSet. You can think of this as similar to a RAG operator\n",
    "except it works on data larger than the context window might allow (by heirarchically summarizing summaries). To call it, pass in a docset and a \n",
    "Summarizer like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb772c3-bea1-432f-972e-0e060de01ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sycamore.transforms.summarize import MultiStepDocumentSummarizer, EtCetera\n",
    "from sycamore.query.execution.operations import summarize_data\n",
    "from sycamore.functions.tokenizer import OpenAITokenizer\n",
    "from sycamore.llms.llms import LLMMode\n",
    "\n",
    "oaitk = OpenAITokenizer(OpenAIModels.GPT_4O.value.name, max_tokens=100_000)\n",
    "\n",
    "# Some parameters that will go into the prompt - the question you want the llm to answer, and a description of\n",
    "# the data being provided\n",
    "question = \"For each of the companies mentioned please summarize the impact of mergers and acquisitions on earnings.\"\n",
    "data_desc = \"Acquisition Earnings\"\n",
    "\n",
    "# Construct the summarizer:\n",
    "# llm_mode tells it to make its llm calls asynchronously\n",
    "# fields is a list of fields to include for each document. a list terminating with the sentinel value EtCetera means\n",
    "#     \"all fields, but any that were specified go first\"\n",
    "# tokenizer is needed to determine how many documents can fit in a batch.\n",
    "summarizer = MultiStepDocumentSummarizer(\n",
    "    llm=llm, \n",
    "    llm_mode=LLMMode.ASYNC, \n",
    "    question=question, \n",
    "    data_description=data_desc, \n",
    "    fields=[EtCetera],\n",
    "    tokenizer=oaitk\n",
    ")\n",
    "\n",
    "# Summarize:\n",
    "# Give it \n",
    "summary = summarize_data(\n",
    "    llm=oai,\n",
    "    question=question,\n",
    "    data_description=data_desc,\n",
    "    input_data=[acq_elts],\n",
    "    docset_summarizer=summarizer\n",
    ")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984bdf4-9518-41ba-b8b1-e8a0a9db5c30",
   "metadata": {},
   "source": [
    "### Question 8: List all the companies that mentioned inflation and give me a count of the number of times each of the companies mentioned inflation.\n",
    "\n",
    "Simple RAG cannot answer this question. I might be wrong, I guess, but I couldn't get it to work. And I think any RAG solution\n",
    "that does work would be very unnatural. Let's see what this looks like with sycamore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3072945-cc16-4785-86d5-ab5a08855459",
   "metadata": {},
   "outputs": [],
   "source": [
    "question8 = \"List all the companies that mentioned inflation and give me a count of the number of times each of the companies mentioned inflation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdb32b-7dce-4f97-86b5-0b6d9afddc54",
   "metadata": {},
   "source": [
    "First off, we'll want to operate on elements, not documents, since we would like to count the number of inflation mentions in each document.\n",
    "Doing this with sycamore is actually quite simple, although I tend to avoid it where possible as it makes the data model harder to reason about.\n",
    "Remember: Each member of a DocSet is a Document, and each Document contains several Elements. We're about to make sycamore treat each Element as\n",
    "a Document.\n",
    "\n",
    "The `docset.explode()` transform turns every Element into a top-level Document. However, it keeps the original top-level Documents as husks of their\n",
    "former selves (they contain no elements). Accordingly, we'll throw in this filter at the end to get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364bce5-c2d0-44c2-bfa9-e53233e24783",
   "metadata": {},
   "outputs": [],
   "source": [
    "element_docset = cleaned_docset.explode().filter(lambda doc: \"parent_id\" in doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dddddd9-a90a-4cc5-ba21-40cff4c26968",
   "metadata": {},
   "source": [
    "Next we ask an LLM whether each element-now-document mentions inflation, keeping only the ones that do.\n",
    "We will be left with a DocSet full of quotes that mention inflation, so we can group them by company and\n",
    "then count, using the `groupby_count` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31894a4-844d-4220-8acf-3a69a51ce49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.llms.prompts.default_prompts import LlmFilterMessagesJinjaPrompt\n",
    "from sycamore.llms.llms import LLMMode\n",
    "from sycamore.functions.tokenizer import OpenAITokenizer\n",
    "\n",
    "tk = OpenAITokenizer(model_name=OpenAIModels.GPT_4O.value.name)\n",
    "\n",
    "inflation_mentions_ds = element_docset.llm_filter(\n",
    "    llm=llm,\n",
    "    new_field=\"inflation_mentioned_confidence\",\n",
    "    prompt = LlmFilterMessagesJinjaPrompt.fork(filter_question=\"Does this text mention inflation?\", use_elements=False),\n",
    "    tokenizer = tk,\n",
    "    max_tokens = 80_000,\n",
    ").groupby_count('properties.entity.company_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80e561-9c94-45b2-8f7e-4c6a1137f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hahahahahahaha the return of the rich table\n",
    "inflation_table = rich.table.Table(title=\"inflation_mentions\")\n",
    "inflation_table.add_column(\"company\")\n",
    "inflation_table.add_column(\"mentions\")\n",
    "\n",
    "counts = [(d.properties['count'], d.properties['key']) for d in inflation_mentions_ds.take_all()]\n",
    "for c, k in sorted(counts):\n",
    "    inflation_table.add_row(k, str(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a47b9b-6de8-4132-9d65-323d1b8b90c9",
   "metadata": {},
   "source": [
    "## Question 10: Summarize how AI integration is progressing across each company's products.\n",
    "\n",
    "Plan:\n",
    "1. llm_map to get how AI is being integrated into the company's products\n",
    "2. llm_filter \"Is artificial intelligence being discussed?\"\n",
    "3. summarize_data: pointing `fields` at the date, company name, and integration summary properties\n",
    "\n",
    "See if you can get this one to work. Good Luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4860ef4-6c38-435f-bdd5-98f18d990b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
