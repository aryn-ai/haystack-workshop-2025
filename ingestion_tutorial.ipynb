{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58f4e3-7aa9-46b0-a28c-7b760f04d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's explore the data. By now you should have downloaded some pdf files, so let's look at one of them.\n",
    "# This also tests that you have poppler installed.\n",
    "from error_messages import *\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "pdf_dir = repo_root / \"files\" / \"actual_files\"\n",
    "one_pdf = next(pdf_dir.iterdir())\n",
    "one_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa8c79-8723-4337-92dd-190a5ca0e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "try:\n",
    "    ims = convert_from_path(one_pdf)\n",
    "    display(ims[0])\n",
    "except Exception as e:\n",
    "    poppler_failed_error()\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed807ca-2e2d-4f74-be9f-c241a3f9a2f7",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "We've found that in order to process documents well, it is important to break the document up into cohesive _elements_. There are a number of strategies for doing this, but we've found success with visual models. For this workshop we'll use Aryn DocParse for this first step. Get started with aryn_sdk. This will also make sure your credentials are set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92184ee-d8a3-408f-b166-382e1fd24541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.partition import partition_file\n",
    "\n",
    "try:\n",
    "    data = partition_file(one_pdf)\n",
    "    elements = data['elements']\n",
    "except Exception as e:\n",
    "    aryn_no_api_key_error()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249f903-c3ad-4399-a118-300196038852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can visualize the elements by drawing the bounding boxes onto the pdf. aryn_sdk has a function for that.\n",
    "from aryn_sdk.partition import draw_with_boxes\n",
    "\n",
    "images = draw_with_boxes(one_pdf, data)\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dd659-4953-49c7-b320-9b8eb68720bd",
   "metadata": {},
   "source": [
    "Each element contains a bunch of information. Core information includes `type`, `bbox`, and `text_representation`. Additional information is stored in a `properties` dict, such as the page number the element is on.\n",
    "\n",
    "Let's have a quick quiz to introduce elements. I've created a bunch of functions that operate on the list of elements returned by the partitioner. Your job is to implement them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795bdbe-eb43-48f9-bbad-15aee479f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_page_headers(elts: list[dict]) -> int:\n",
    "    \"\"\"Return the number of elements of type 'Page-header'\"\"\"\n",
    "    return 64\n",
    "    raise NotImplementedError(\"Finish this yourself\")\n",
    "    \n",
    "def number_of_elements_after_page_4(elts: list[dict]) -> int:\n",
    "    \"\"\"Return the number of elements that fall after page 4. Note that page numbers are 1-indexed.\"\"\"\n",
    "    return 285\n",
    "    raise NotImplementedError(\"Finish this yourself\")\n",
    "\n",
    "def number_of_jassy_mentions(elts: list[dict]) -> int:\n",
    "    \"\"\"Return the number of elements that contain the string 'Jassy' (Andy Jassy is the CEO of AWS).\n",
    "    Note: some elements do not have a 'text_representation' key.\"\"\"\n",
    "    return 11\n",
    "    raise NotImplementedError(\"Finish this yourself\")\n",
    "\n",
    "def number_of_elements_that_cover_a_third_of_the_page(elts: list[dict]) -> int:\n",
    "    \"\"\"For this you'll need the bbox property. bboxes are represented as 4 floats, [x1, y1, x2, y2]. Each \n",
    "    coordinate ranges from 0 to 1, representing the fraction of the page (left-to-right for x, top-to-bottom for y) \n",
    "    where the point lies. So [0, 0, 1, 1] is the whole page, and [0, 0.5, 0.5, 1] is the lower-left quadrant.\n",
    "    \n",
    "    Return the number of elements that cover at least half of the page. An element covers half the page if its \n",
    "    area is greater than 1/3\"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"Finish this yourself\")\n",
    "    return len([e for e in elts if (e['bbox'][2] - e['bbox'][0]) * (e['bbox'][3] - e['bbox'][1]) > 1/3])\n",
    "\n",
    "\n",
    "assert number_of_page_headers(elements) == 64, f\"Got {number_of_page_headers(elements)}. Make sure your capitalization is correct.\"\n",
    "\n",
    "assert number_of_elements_after_page_4(elements) == 285, f\"Got {number_of_elements_after_page_4(elements)}. If you got 295, 'after page 4' does not include page 4, and page numbers are 1-indexed.\"\n",
    "\n",
    "assert number_of_jassy_mentions(elements) == 11, f\"Got {number_of_jassy_mentions(elements)}. A 'jassy mention' is defined as an element whose text contains the string 'Jassy'.\"\n",
    "\n",
    "assert number_of_elements_that_cover_a_third_of_the_page(elements) == 4, f\"Got {number_of_elements_that_cover_a_third_of_the_page(elements)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16fb706-6d55-45ec-9410-0916f3e4c989",
   "metadata": {},
   "source": [
    "## Sycamore basics\n",
    "\n",
    "By now you have a basic sense of the data model - a Document is made up of Elements which represent logical chunks of the Document, and contain additional metadata about themselves.\n",
    "The next step is to scale this past one document to many, and this is where Sycamore comes in. Sycamore adds a data structure called a DocSet, which is a set of Documents.\n",
    "Each Document in the DocSet contains the list of Elements that it comprises, and a bunch of metadata as well (for instance, the name of the file the document came from).\n",
    "\n",
    "Now -- and this is the tricky bit -- a DocSet is not really a container type like you might expect in most systems. It is better to think about a DocSet as a program. Because it is\n",
    "intended to operate at very large scale, we can't necessarily hold all of the documents in memory. This is a tool for writing to databases, and so if you could just have everything\n",
    "in memory at once then you wouldn't need a database in the first place! This is where the program view of a DocSet comes in. Instead of being a materialized set of Document objects\n",
    "(potentially hundreds of thousands of them, each rather large), a DocSet contains a list of processing instructions to generate the Documents in a pipelined, streaming fashion. This\n",
    "means you don't have things you might expect like random access.\n",
    "\n",
    "So how do you build this program? This is covered more in-depth in the next segment of the workshop, but the 10,000 mile view is that DocSet has a bunch of methods, that you can call\n",
    "to add steps to the program. DocSet programs are technically immutable, so you write them like so\n",
    "```python\n",
    "docset = docset.partition(...).map(...)\n",
    "docset = docset.map_elements(...)\n",
    "```\n",
    "instead of something like\n",
    "```python\n",
    "# This doesn't do what you think it does\n",
    "docset.partition(...)\n",
    "docset.map(...)\n",
    "docset.map_elements(...)\n",
    "```\n",
    "\n",
    "In order to execute a DocSet, there are a couple of methods that do that. \n",
    "\n",
    "- `docset.execute()` executes the docset and does nothing with the resulting Documents. Most production pipelines use this to run.\n",
    "- `docset.take_all()` (and its friend `docset.take(n)`) executes the docset and returns the Documents in a plain python list. This is useful for debugging and development, when datasets are still small.\n",
    "- `docset.show()` executes and prints the first couple Documents - good for development\n",
    "- `docset.write.<some_target>()` executes the docset and writes the documents out to some target - could be a database like opensearch, or just the filesystem. Most of these writers have an `execute` flag that determines whether to execute the write (and return nothing) or just return a DocSet with the write in the plan.\n",
    "\n",
    "Each docset is bound to a Sycamore Context, which is the execution engine that actually runs the program. We've implemented 2 execution modes, `LOCAL` and `RAY`. `RAY` mode executes the DocSet on a [ray](https://www.ray.io/) cluster,\n",
    "creating one locally if it does not find an existing ray cluster. This mode scales well, running transforms on Documents in parallel across processes (and nodes if you've set it up), but it can be tricky to debug - distributed \n",
    "processing jobs can be like that. `LOCAL` mode runs in single-threaded python in the process and is generally better for debugging, but you lose the distributed/parallel aspect. For the beginning of the workshop, we will run in \n",
    "`LOCAL` mode, and then transition to `RAY` when we have ironed out the DocSet plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44bf0d-02b7-4f86-b902-1754af0925c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sycamore\n",
    "from sycamore import ExecMode\n",
    "\n",
    "context = sycamore.init(exec_mode = ExecMode.LOCAL)\n",
    "assert context.exec_mode == ExecMode.LOCAL, \"Change the exec mode in the init to LOCAL to use local mode\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7175b1-deef-4cc9-872b-f1a15f00d2e4",
   "metadata": {},
   "source": [
    "To create the DocSet from nothing, we need to tell sycamore how to read in the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002de04a-fa2a-4780-a75e-76bfc44dc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docset = context.read.binary(paths=str(one_pdf), binary_format=\"pdf\")\n",
    "\n",
    "# Let's see what that gave us\n",
    "pdf_docset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50243b3f-53b7-4545-9be7-36fe02340ec9",
   "metadata": {},
   "source": [
    "Our docset has a single Document in it, with a 'properties' dict containing some metadata, an 'elements' list containing an empty list of elements, a doc_id, lineage_id, type, and binary_representation, which contains the binary of the original PDF.\n",
    "To get the elements as before, we'll want to run the `partition` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be01ac-5898-42a0-b911-f3a0895c08a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "\n",
    "# If you did not see the error message about API keys, ignore this comment.\n",
    "# You might need to add aryn_api_key=\"<YOUR KEY>\" if the environment didn't pick it up correctly. \n",
    "partitioned_docset = pdf_docset.partition(ArynPartitioner())\n",
    "\n",
    "# We'll limit the number of elements to show because otherwise this produces an obnoxiously large output cell\n",
    "partitioned_docset.show(num_elements=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfe94d-5a21-403e-8017-54125dac278e",
   "metadata": {},
   "source": [
    "We can explore the Document and Elements in much the same way as before with the raw outputs. I won't make you write a bunch of code this time though. Now that we're in sycamore, the Document and Elements are actual classes with attributes, which makes them a little easier to work with and also explains why I've been capitalizing them the whole time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef39a10-e9a2-4504-afcf-09df4fc898f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = partitioned_docset.take_all()\n",
    "my_doc = docs[0]\n",
    "\n",
    "print(f\"doc id: {my_doc.doc_id}\")\n",
    "print(f\"properties: {my_doc.properties}\")\n",
    "print(f\"number of elements: {len(my_doc.elements)}\")\n",
    "print(f\"number of elements that take up more than 1/3 of the page: {len([e for e in my_doc.elements if e.bbox.area > 1/3])}\")\n",
    "print(f\"final element text: {my_doc.elements[-1].text_representation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbeee31-e40e-4372-bbdc-7c2675bb0386",
   "metadata": {},
   "source": [
    "Wait a second.\n",
    "\n",
    "Running `show` and running `take_all` ran the whole program all over again! This could get really cumbersome to work with. I have a solution for you: `materialize`.\n",
    "\n",
    "Doing everything all over again is by design. As explained previously, we can't always hold everything in memory, so we have to re-execute stuff when we want to reference it again.\n",
    "However, we can effectively use disk as a cache with the `materialize` method. When sycamore compiles a DocSet into an execution plan, it starts from the end of the DocSet and works\n",
    "toward the beginning. When it sees a `materialize`, it looks at the location where the `materialize` thinks its cache lives, and if it finds data, it finishes compiling and reads the\n",
    "data in, essentially truncating the program to only what comes after it. If it doesn't find data, it continues compiling, and adds a step to write to the location. \n",
    "\n",
    "TLDR; `docset.materialize(path=\"filesystem/directory\", source_mode=MaterializeSourceMode.USE_STORED)` makes the docset up until the materialize execute only once and then cache the data \n",
    "at that point in the execution for any future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2ecd9-e148-4dbb-97e4-ae4e2d90bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.materialize import MaterializeSourceMode\n",
    "\n",
    "materialize_dir = repo_root / \"materialize\"\n",
    "\n",
    "materialized_ds = partitioned_docset.materialize(path = materialize_dir / \"onedoc-partitioned\", source_mode = MaterializeSourceMode.USE_STORED)\n",
    "\n",
    "materialized_ds.execute()\n",
    "print(\"Finished executing the first time\")\n",
    "print(\"=\" * 80)\n",
    "# Note that the second time this is fast\n",
    "materialized_ds.execute()\n",
    "print(\"Finished executing the second time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea954b6-62f9-4d1e-8fd6-b27e26a28579",
   "metadata": {},
   "source": [
    "Sometimes, you'll want to redo a step that's been materialized. The simplest option is to remove the directory with all the cached data, e.g. `rm -rd materialize/onedoc-partitioned`\n",
    "\n",
    "Now that we have the docset materialized, let's use a utility function to visualize the bounding boxes like we did with `aryn_sdk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d94aa-f735-47cf-b740-bcad543760be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sycamore.utils.pdf_utils import show_pages\n",
    "\n",
    "show_pages(materialized_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0f69d-6bfe-46aa-b283-f2d60ddb9d18",
   "metadata": {},
   "source": [
    "I don't know who Lou and Travis are but they're getting some good exposure here. Anyway.\n",
    "\n",
    "## Property Extraction\n",
    "\n",
    "One of sycamore's biggest benefits is its ability to interact with LLMs in this kind of data-flow-y way. LLMs are good at understanding unstructured data, so for processing unstructured\n",
    "documents, they're a very useful tool. They make it easy to extract common metadata properties from documents, and with sycamore we can very easily apply this to all documents in a docset.\n",
    "\n",
    "First, we'll want to clean up some of the elements that are not going to be useful. For example, we have all these page headers and footers and captions. Let's get rid of them using `docset.filter_elements`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae6f74-acba-47cc-a8aa-b76c43966d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.data import Element\n",
    "\n",
    "def filter_out_annoying_elements(elt: Element) -> bool:\n",
    "    return elt.type not in (\"Page-header\", \"Page-footer\", \"Caption\")\n",
    "\n",
    "# docset.filter_elements takes a predicate function that maps Elements to bools. \n",
    "# For each element in a document, keep the element only if predicate(element) is True.\n",
    "filtered_ds = materialized_ds.filter_elements(filter_out_annoying_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bb621-40e7-4a27-8997-2cf697c72ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.llms.openai import OpenAI, OpenAIModels\n",
    "from sycamore.transforms.extract_schema import LLMPropertyExtractor\n",
    "\n",
    "# You might need to explicitly set an api key here if it's not picked up from the environment variables\n",
    "# Add api_key = \"<key>\"\n",
    "gpt4o = OpenAI(OpenAIModels.GPT_4O)\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"quarter\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Quarter of the earnings call, it should be in the format of Q1, Q2, Q3, Q4\",\n",
    "        },\n",
    "        \"date\":{\"type\": \"string\", \"description\": \"The date of the earnings call\"}\n",
    "    },\n",
    "}\n",
    "\n",
    "# Quiz: As is, this property extraction will never run, even if I do something like `filtered_ds.execute()`. Why?\n",
    "filtered_ds.extract_properties(LLMPropertyExtractor(llm=gpt4o, schema=schema, schema_name=\"earnings_call\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2f735-64be-40de-a53a-ddf4e67d7423",
   "metadata": {},
   "source": [
    "Now see if you can add a `company_name` and `company_ticker` property to this schema and extract properties into a docset named `extracted_ds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19c277-27e5-48cc-b97d-0de81ec7a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"quarter\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Quarter of the earnings call, it should be in the format of Q1, Q2, Q3, Q4\",\n",
    "        },\n",
    "        \"date\":{\"type\": \"string\", \"description\": \"The date of the earnings call\"},\n",
    "\n",
    "... # Fill in the rest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a05fa-e102-4b1d-9849-3b537facd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the schema is right. We'll reference these properties later.\n",
    "for doc in extracted_ds.take(1):\n",
    "    assert 'earnings_call' in doc.properties\n",
    "    ec = doc.properties['earnings_call']\n",
    "    assert 'date' in ec\n",
    "    assert 'quarter' in ec\n",
    "    assert 'company_name' in ec\n",
    "    assert 'company_ticker' in ec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7379f-03e6-4c35-bedf-0a96b8e03f16",
   "metadata": {},
   "source": [
    "Great! Now is there anything you can do to prevent yourself from making this LLM call over and over?\n",
    "\n",
    "## Chunking\n",
    "\n",
    "Another operation you'll probably want to run a lot when doing unstructured data preparation is chunking. That is, we've broken the text up into a whole lot of tiny little bits, but when we actually \n",
    "embed it and index it for search we'll likely want slightly larger chunks. Sycamore implements a number of chunking strategies (documentation [here](https://sycamore.readthedocs.io/en/stable/sycamore/APIs/low_level_transforms/merge_elements.html)). \n",
    "For this workshop we will use the `MarkedMerger` as it is the most customizable.\n",
    "\n",
    "Our data is an earnings call, so there is a very natural way to chunk that - for each speaker 'block' we should get a chunk. In our partitioning we have split the text into paragraphs, but we'd like \n",
    "to squish all those paragraphs together, breaking the blocks wherever there's a new speaker. With a little bit of effort we can detect the lines that introduce speakers with regexes - one for external\n",
    "speakers and one for internal speakers, as the formatting is very consistent (this applies across all the documents in the dataset, don't worry):\n",
    "\n",
    "```python\n",
    "external_re = '([^ ]*[^\\S\\n\\t]){1,4}--[^\\S\\n\\t].*--' # A name (1-4 words long) followed by -- followed by anything followed by --\n",
    "internal_re = '([^ ]*[^\\S\\n\\t]){1,4}--.*'            # A name (1-4 words long) followed by -- followed by anything\n",
    "```\n",
    "\n",
    "The `MarkedMerger` is set up perfectly to work with this. It will step through the elements, merging them together one by one, unless it sees one of two 'marks' in the data:\n",
    "\n",
    "- on a \"_drop\" mark it drops the element and continues merging\n",
    "- on a \"_break\" mark it finalizes the merged element and uses this one to start merging up a new element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1137a8c-936a-4748-96d3-a84d548e4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "\n",
    "def markSpeakers(elt: Element) -> Element:\n",
    "    if not elt.text_representation:\n",
    "        return elt\n",
    "\n",
    "    external_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--[^\\S\\n\\t].*--', elt.text_representation)\n",
    "    internal_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--.*', elt.text_representation)\n",
    "    if elt.text_representation.strip() == 'Operator':\n",
    "        # The operator is also a speaker! In this case, we should set\n",
    "        # the 'speaker' property to True and the 'speaker_role' and \n",
    "        # 'speaker_name' properties to 'Operator'. We should also tell\n",
    "        # the MarkedMerger to break.\n",
    "        raise NotImplementedError(\"I thought operators were an algebra thing!\")\n",
    "    elif external_speaker:\n",
    "        location = elt.text_representation.find('--')\n",
    "        location2 = location + elt.text_representation[location+2:].find('--')\n",
    "        elt.properties['speaker_name'] = elt.text_representation[:location].strip()\n",
    "        elt.properties['speaker_external_org'] = elt.text_representation[location+2:location2+1].strip()\n",
    "        elt.properties['speaker_role'] = elt.text_representation[location2+4:].strip()\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    elif internal_speaker:\n",
    "        location = elt.text_representation.find('--')\n",
    "        elt.properties['speaker_name'] = elt.text_representation[:location].strip()\n",
    "        elt.properties['speaker_role'] = elt.text_representation[location+2:].strip()\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    return elt\n",
    "\n",
    "speakers_marked_ds = filtered_docset.map_elements(markSpeakers)\n",
    "merged_ds = speakers_merged_ds.merge_elements(MarkedMerger())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e9e0f-7f7d-43ba-90af-5580a6bb71a1",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "Embedding data with sycamore is fairly simple, so I'm going to give you all the information you need to do it and let you write it out.\n",
    "There is a method on DocSets called `embed()`. It takes an `Embedder` as its parameter. We'll use the `OpenAIEmbedder`, which you can\n",
    "import from `sycamore.transforms.embed`. It takes a `model_name` parameter which we'll set to `'text-embedding-3-small'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bffcfe-6a17-46f2-b55b-5515d88c9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "embedded_ds = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d50874-867c-4256-aa16-22acac62c8eb",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "We'll be writing our data to Aryn (because what kind of workshop would this be if we didn't stand behind our own data warehouse). Sycamore can\n",
    "also write to a number of other systems, such as OpenSearch, ElasticSearch, Weaviate, etc. Each one has some small idiosyncracies in what's \n",
    "needed to write successfully, and Aryn is no different. I'm going to fix a lot of these issues when I get home but for now just bear with me.\n",
    "\n",
    "The unit of storage in Aryn equivalent to an index in OpenSearch or a table in a SQL DB is a 'DocSet.' While a Sycamore DocSet is usually best \n",
    "understood as a program, an Aryn DocSet is actually a container. We can create one using aryn_sdk, and then write our (sycamore) docset to it.\n",
    "\n",
    "However, first we need to move around some properties. This is one of those kinda ugly things I intend to fix. This is a literary technique \n",
    "known as lampshading. That said, there is some learning to be had here.\n",
    "\n",
    "Probably the most useful docset method is `map` (and it's corollary `map_elements`). `map` accepts a function with a `Document` input and `Document` \n",
    "output, and calls that function on every Document in the DocSet. (`map_elements` does the same but for all the Elements of every Document). Basically\n",
    "every transform is a map. \n",
    "\n",
    "Here, I'll create a higher-order function that captures a list of properties, and returns a `map` function (`Document`->`Document`) that moves all of\n",
    "those properties into a nested dictionary `properties['entity']`. In fact I've been a little sneaky - since python is dynamically typed and `Document`s \n",
    "and `Element`s both have a `properties` dict, I can use this in a `map_elements` as well... just as long as I don't run mypy on it!\n",
    "\n",
    "I'll also add in a `spread_properties` transform, which copies properties from every Document to each of its Elements, so that all the elements have\n",
    "the `earnings_call` and `path` metadata associated with the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b60b36-498e-40d3-a6c0-89dab05203f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transform moves a bunch of properties to a nested dict properties.entity. \n",
    "# Useful for making sure some downstream processing works correctly\n",
    "def prop_to_entity(props: list[str]):\n",
    "    def prop_to_entity_inner(doc_or_elt):\n",
    "        if \"entity\" not in doc_or_elt.properties:\n",
    "            doc_or_elt.properties[\"entity\"] = {}\n",
    "        if not isinstance(doc_or_elt.properties[\"entity\"], dict):\n",
    "            doc_or_elt.properties[\"entity\"] = {\"original_entity\": doc_or_elt.properties.pop(\"entity\")}\n",
    "        for p in props:\n",
    "            if p in doc_or_elt.properties:\n",
    "                doc_or_elt.properties[\"entity\"][p] = doc_or_elt.properties.pop(p)\n",
    "        return doc_or_elt\n",
    "    return prop_to_entity_inner\n",
    "\n",
    "# Also here's a nice way of writing chained pipelines\n",
    "rejiggered_ds = (\n",
    "    embedded_ds\n",
    "    .map(prop_to_entity(['earnings_call']))\n",
    "    .spread_properties(['path', 'entity'])\n",
    "    .map_elements(prop_to_entity(['speaker', 'speaker_name', 'speaker_role', 'speaker_external_org']))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290188d-da80-4642-9b41-056b7509e4fe",
   "metadata": {},
   "source": [
    "Now let's create our docset target (give it a name) and write to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3831d1-f191-4ad4-970e-91c557cb7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.client.client import Client\n",
    "\n",
    "# You may need to specify aryn_api_key=\"<YOUR KEY>\" here\n",
    "aryn_client = Client()\n",
    "\n",
    "docset_name = \n",
    "aryn_docset = aryn_client.create_docset(name = docset_name)\n",
    "\n",
    "print(aryn_docset.value.docset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438ca2a-a544-41c9-8037-bdc34b74e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to specify aryn_api_key=\"<YOUR KEY>\" here too.\n",
    "rejiggered_ds.write.aryn(docset_id=docset.value.docset_id, autoschema=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
