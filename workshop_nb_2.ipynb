{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16fb706-6d55-45ec-9410-0916f3e4c989",
   "metadata": {},
   "source": [
    "# Workshop Notebook 2 - DocSets and Document Processing\n",
    "\n",
    "In this notebook, we will scale from one document to two, using sycamore to apply various forms of processing to each of them, in order to write them to a database\n",
    "and be able to answer questions like:\n",
    "\n",
    "0. In the Broadcom earnings call, what details did the CFO, Kirsten Spears, discuss about the VMware acqusition?\n",
    "\n",
    "## Sycamore basics\n",
    "\n",
    "By now you have a basic sense of the data model - a Document is made up of Elements which represent logical chunks of the Document, and contain additional metadata about themselves.\n",
    "The next step is to scale this past one document to many, and this is where Sycamore comes in. Sycamore adds a data structure called a DocSet, which is a set of Documents.\n",
    "Each Document in the DocSet contains the list of Elements that it comprises, and a bunch of metadata as well (for instance, the name of the file the document came from).\n",
    "\n",
    "Now you'll likely want to apply a series of transformations to the Documents before you write them to a database. You can imagine writing a big for loop over all the documents and\n",
    "calling a series of functions on them in order. Maybe you throw `multiprocessing` at it to parallelize it. Maybe you run nested loops to do some sort of batching. You have to do a \n",
    "lot of work to optimize it, and you still probably aren't using memory as efficiently as you could be. \n",
    "\n",
    "DocSets make processing large amounts of documents easy. DocSet methods are mostly processing steps to be applied to every document in the DocSet - so instead of writing\n",
    "```python\n",
    "# without docsets\n",
    "processed_documents = []\n",
    "for document in list_of_documents:\n",
    "    processed_documents.append(foo(document))\n",
    "```\n",
    "You can write\n",
    "```python\n",
    "# with docsets\n",
    "processed_docset = docset.map(foo)\n",
    "```\n",
    "\n",
    "### Execution modes\n",
    "\n",
    "Each docset is bound to a Sycamore Context, which is the execution engine that actually runs the program. We've implemented 2 execution modes, `LOCAL` and `RAY`. `RAY` mode executes the DocSet on a [ray](https://www.ray.io/) cluster,\n",
    "creating one locally if it does not find an existing ray cluster. This mode scales well, running transforms on Documents in parallel across processes (and nodes if you've set it up), but it can be tricky to debug - distributed \n",
    "stack traces are notoriously unwieldy. `LOCAL` mode runs in single-threaded python in the process and is generally better for debugging, but you lose the distributed/parallel aspect. For the beginning of the workshop, we will run in \n",
    "`LOCAL` mode, and then transition to `RAY` when we have ironed out the DocSet plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44bf0d-02b7-4f86-b902-1754af0925c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sycamore\n",
    "from sycamore import ExecMode\n",
    "\n",
    "context = sycamore.init(exec_mode = ExecMode.RAY)\n",
    "assert context.exec_mode == ExecMode.LOCAL, \"Change the exec mode in the init to LOCAL to use local mode\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7175b1-deef-4cc9-872b-f1a15f00d2e4",
   "metadata": {},
   "source": [
    "To create the DocSet from nothing, we need to tell sycamore how to read in the initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002de04a-fa2a-4780-a75e-76bfc44dc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd()\n",
    "pdf_dir = repo_root / \"files\" / \"earnings_calls\"\n",
    "two_pdfs = [str(pdf_dir / \"broadcom-avgo-q1-2024-earnings-call-transcript.pdf\"), str(pdf_dir / \"mongodb-mdb-q1-2024-earnings-call-transcript.pdf\")]\n",
    "\n",
    "pdf_docset = context.read.binary(paths=two_pdfs, binary_format=\"pdf\")\n",
    "\n",
    "# Let's see what that gave us\n",
    "pdf_docset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50243b3f-53b7-4545-9be7-36fe02340ec9",
   "metadata": {},
   "source": [
    "Our docset has two Documents in it, with a 'properties' dict containing some metadata, an 'elements' list containing an empty list of elements, a doc_id, lineage_id, type, and binary_representation, which contains the binary of the original PDF.\n",
    "To get the elements as before, we'll want to run the `partition` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be01ac-5898-42a0-b911-f3a0895c08a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sycamore.transforms.partition import ArynPartitioner\n",
    "\n",
    "# If you did not see the error message about API keys, ignore this comment.\n",
    "# You might need to add aryn_api_key=\"<YOUR KEY>\" if the environment didn't pick it up correctly. \n",
    "partitioned_docset = pdf_docset.partition(ArynPartitioner())\n",
    "\n",
    "# We'll limit the number of elements to show because otherwise this produces an obnoxiously large output cell\n",
    "partitioned_docset.show(num_elements=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfe94d-5a21-403e-8017-54125dac278e",
   "metadata": {},
   "source": [
    "We can visualize bounding boxes in much the same way that we did with aryn_sdk, with sycamore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49c688-4c5d-4050-907f-e5952a4515a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.utils.pdf_utils import show_pages\n",
    "\n",
    "show_pages(partitioned_docset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544ac101-5169-4a0c-b27f-122e80be0141",
   "metadata": {},
   "source": [
    "## Sycamore UDFs\n",
    "\n",
    "Inspecting the elements and their types, we can confirm that in order to answer questions about the content of our documents (\"What details did the CFO, Kirsten Spears, discuss about the VMware acqusition?\") we only need to keep around Section-headers and Text elements. For other workloads this probably doesn't apply, but here it provides a lovely opportunity to demonstrate one of the four most useful docset udf-transforms, `filter_elements`. Here's the list of udf transforms:\n",
    "\n",
    "- `docset.map(f)`: Applies a function (`Document` -> `Document`) to every Document in the DocSet\n",
    "- `docset.map_elements(f)`: Applies a function (`Element` -> `Element`) to every Element in every Document in the DocSet\n",
    "- `docset.filter(f)`: Applies a predicate function (`Document` -> `bool`) to every Document, keeping only those Documents for which f(Document) is True\n",
    "- `docset.filter_elements(f)`: Applies a predicate function (`Element` -> `bool`) to every Element in every Document, keeping only Elements for which f(Element) is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173dfbdf-48c2-4004-bbcd-ae8097ac870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.data import Element\n",
    "\n",
    "def keep_only_text_and_sh(elt: Element) -> bool:\n",
    "    return elt.type in (\"Section-header\", \"Text\")\n",
    "\n",
    "# docset.filter_elements takes a predicate function that maps Elements to bools. \n",
    "# For each element in a document, keep the element only if predicate(element) is True.\n",
    "filtered_ds = partitioned_docset.filter_elements(keep_only_text_and_sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbeee31-e40e-4372-bbdc-7c2675bb0386",
   "metadata": {},
   "source": [
    "Wait a second.\n",
    "\n",
    "Running `showPages` and `show` ran the whole program all over again! This could get really cumbersome to work with, especially as we add additional transforms to our processing\n",
    "pipeline in development. I have a solution for you: `materialize`.\n",
    "\n",
    "DocSets are evaluated lazily, which means that as you're developing, the only thing held in memory in the DocSet object itself is an execution plan. To get the data in the DocSet,\n",
    "you have to 'execute' it - i.e. tell sycamore to run all the steps in the execution plan, from reading in the data to each transform. This allows sycamore to apply these sorts of \n",
    "parallelization/batch/streaming optimizations without you having to think about them. However, it comes with a drawback - accessing the documents themselves for ad-hoc inspection\n",
    "can be a little bit difficult. For example, DocSets do not provide random access to data.\n",
    "\n",
    "In order to execute a DocSet, there are a couple of methods that do that. \n",
    "\n",
    "- `docset.execute()` executes the docset and does nothing with the resulting Documents. Most production pipelines use this to run.\n",
    "- `docset.take_all()` (and its friend `docset.take(n)`) executes the docset and returns the Documents in a plain python list. This is useful for debugging and development, when datasets are still small.\n",
    "- `docset.count()` executes the docset and returns the number of Documents in it. This is most useful when debugging filters (map transforms don't change the size of the docset).\n",
    "- `docset.show()` executes and prints the first couple Documents - good for development\n",
    "- `docset.write.<some_target>()` executes the docset and writes the documents out to some target - could be a database like opensearch, or just the filesystem. Most of these writers have an `execute` flag that determines whether to execute the write (and return nothing) or just return a DocSet with the write in the plan.\n",
    "\n",
    "### Debugging & Materialize\n",
    "\n",
    "Debugging with sycamore can be tricky, but it's possible with a little bit of creativity. First-off, the execution-forcing methods above are useful - particularly `take` and `take_all` since they give you back the Documents.\n",
    "Printing data can be a little hard to find as the log streams tend to get fairly polluted with stuff, so I will sometimes simply write a function that writes a piece of data for every Document/Element to a file and apply it \n",
    "with a `map` or `map_elements` like so:\n",
    "```python\n",
    "def write_debug(doc):\n",
    "    with open(\"debug.txt\", \"a\") as f:\n",
    "        f.write(f\"Document {doc.doc_id}\\n\")\n",
    "        f.write(json.dumps(doc.elements, indent=2))\n",
    "        f.write(\"\\n\" + '-' * 80 + \"\\n\")\n",
    "    return doc\n",
    "\n",
    "docset.map(write_debug)\n",
    "```\n",
    "\n",
    "We can effectively use disk as a cache with the `materialize` method. When sycamore compiles a DocSet into an execution plan, it starts from the end of the DocSet and works toward \n",
    "the beginning. When it sees a `materialize`, it looks at the location where the `materialize` thinks its cache lives, and if it finds data, it finishes compiling and reads the data \n",
    "in, essentially truncating the program to only what comes after it. If it doesn't find data, it continues compiling, and adds a step to write to the location. \n",
    "\n",
    "TLDR; `docset.materialize(path=\"filesystem/directory\", source_mode=MaterializeSourceMode.USE_STORED)` makes the docset up until the materialize execute only once and then cache the data \n",
    "at that point in the execution for any future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2ecd9-e148-4dbb-97e4-ae4e2d90bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.materialize import MaterializeSourceMode\n",
    "\n",
    "materialize_dir = repo_root / \"materialize\"\n",
    "\n",
    "materialized_ds = filtered_ds.materialize(path = materialize_dir / \"twodocs-partitioned\", source_mode = MaterializeSourceMode.USE_STORED)\n",
    "\n",
    "materialized_ds.execute()\n",
    "print(\"Finished executing the first time\")\n",
    "print(\"=\" * 80 + \"\\n\\n\")\n",
    "# Note that the second time this is fast\n",
    "materialized_ds.execute()\n",
    "print(\"Finished executing the second time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea954b6-62f9-4d1e-8fd6-b27e26a28579",
   "metadata": {},
   "source": [
    "Sometimes, you'll want to redo a step that's been materialized. The simplest option is to remove the directory with all the cached data, e.g. `rm -rd materialize/onedoc-partitioned`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0f69d-6bfe-46aa-b283-f2d60ddb9d18",
   "metadata": {},
   "source": [
    "## Schema Extraction\n",
    "\n",
    "Ok, let's go back to the question we were trying to answer:  \"In the Broadcom earnings call, what details did the CFO, Kirsten Spears, discuss about the VMware acquisition?\" Now notice that we have two documents loaded into our DocSet, a Broadcom earnings call and a MongoDB earnings call. To successfully answer this question we'll have to perform the following steps:\n",
    "\n",
    "1. Identify the Broadcom document\n",
    "2. Identify the elements where Kirsten Spears is speaking\n",
    "3. Identify the element where she mentions VMWare. \n",
    "\n",
    "Now notice that the first step requires identifying the Broadcom document. A reasonable way to accomplish this is to extract the company from each document. Then we can filter the \n",
    "documents like we did the elements.\n",
    "\n",
    "One of sycamore's biggest benefits is its ability to interact with LLMs in this kind of data-flow-y way. LLMs are good at understanding unstructured data, so for processing unstructured\n",
    "documents, they're a very useful tool. They make it easy to extract common metadata properties from documents, and with sycamore we can very easily apply this to all documents in a docset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bb621-40e7-4a27-8997-2cf697c72ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sycamore.llms.openai import OpenAI, OpenAIModels\n",
    "from sycamore.transforms.extract_schema import LLMPropertyExtractor\n",
    "\n",
    "# You might need to explicitly set an api key here if it's not picked up from the environment variables\n",
    "# Add parameter: api_key = \"<key>\"\n",
    "gpt4o = OpenAI(OpenAIModels.GPT_4O)\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"quarter\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Quarter of the earnings call, it should be in the format of Q1, Q2, Q3, Q4\",\n",
    "        },\n",
    "        \"date\":{\"type\": \"string\", \"description\": \"The date of the earnings call\"}\n",
    "    },\n",
    "}\n",
    "\n",
    "# Quiz: As is, this property extraction will never run, even if I do something like `filtered_ds.execute()`. Why?\n",
    "materialized_ds.extract_properties(LLMPropertyExtractor(llm=gpt4o, schema=schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f2f735-64be-40de-a53a-ddf4e67d7423",
   "metadata": {},
   "source": [
    "Now see if you can add a `company_name` and `company_ticker` property to this schema and extract properties into a docset named `extracted_ds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19c277-27e5-48cc-b97d-0de81ec7a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"quarter\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Quarter of the earnings call, it should be in the format of Q1, Q2, Q3, Q4\",\n",
    "        },\n",
    "        \"date\":{\"type\": \"string\", \"description\": \"The date of the earnings call\"},\n",
    "\n",
    "        ... # Fill in the rest!\n",
    "\n",
    "extracted_ds = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a05fa-e102-4b1d-9849-3b537facd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the schema is right. We'll reference these properties later.\n",
    "for doc in extracted_ds.take(1):\n",
    "    print(doc.properties)\n",
    "    assert 'entity' in doc.properties\n",
    "    ec = doc.properties['entity']\n",
    "    assert 'date' in ec\n",
    "    assert 'quarter' in ec\n",
    "    assert 'company_name' in ec\n",
    "    assert 'company_ticker' in ec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f7379f-03e6-4c35-bedf-0a96b8e03f16",
   "metadata": {},
   "source": [
    "Great! Now is there anything you can do to prevent yourself from making this LLM call over and over?\n",
    "\n",
    "## Chunking\n",
    "\n",
    "Now, for our question answering system to be able to detect that this is element where Kristen Spears discusses the VMWare acquistion, we'll need a way to associate the \"speaker element\" that is a few paragraphs above it, with this last element. The way to do that is through chunking. Sycamore implements a number of chunking strategies (documentation [here](https://sycamore.readthedocs.io/en/stable/sycamore/APIs/low_level_transforms/merge_elements.html)). \n",
    "For this workshop we will use the `MarkedMerger` as it is the most customizable.\n",
    "\n",
    "As mentioned before, to be able to answer questions like the one about Kristen Spears we'll chunk such that for each speaker 'block' we  get a chunk. In our partitioning we have split the text into paragraphs, but we'd like to squish all those paragraphs together, breaking the blocks wherever there's a new speaker. With a little bit of effort we can detect the lines that introduce speakers with regexes - one for external speakers and one for internal speakers, as the formatting is very consistent (this applies across all the documents in the dataset, don't worry):\n",
    "\n",
    "```python\n",
    "external_re = '([^ ]*[^\\S\\n\\t]){1,4}--[^\\S\\n\\t].*--' # A name (1-4 words long) followed by -- followed by anything followed by --\n",
    "internal_re = '([^ ]*[^\\S\\n\\t]){1,4}--.*'            # A name (1-4 words long) followed by -- followed by anything\n",
    "```\n",
    "\n",
    "The `MarkedMerger` is set up perfectly to work with this. It will step through the elements, merging them together one by one, unless it sees one of two 'marks' in the data:\n",
    "\n",
    "- on a \"_drop\" mark it drops the element and continues merging\n",
    "- on a \"_break\" mark it finalizes the merged element and uses this one to start merging up a new element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1137a8c-936a-4748-96d3-a84d548e4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sycamore.transforms.merge_elements import MarkedMerger\n",
    "\n",
    "def mark_speakers(elt: Element) -> Element:\n",
    "    if not elt.text_representation:\n",
    "        return elt\n",
    "\n",
    "    external_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--[^\\S\\n\\t].*--', elt.text_representation)\n",
    "    internal_speaker = re.match('([^ ]*[^\\S\\n\\t]){1,4}--.*', elt.text_representation)\n",
    "    if elt.text_representation.strip() == 'Operator':\n",
    "        # The operator is also a speaker! In this case, we should set\n",
    "        # the 'speaker' property to True and the 'speaker_role' and \n",
    "        # 'speaker_name' properties to 'Operator'. We should also tell\n",
    "        # the MarkedMerger to break.\n",
    "        raise NotImplementedError(\"I thought operators were an algebra thing!\")\n",
    "    elif external_speaker:\n",
    "        location = elt.text_representation.find('--')\n",
    "        location2 = location + elt.text_representation[location+2:].find('--')\n",
    "        elt.properties['speaker_name'] = elt.text_representation[:location].strip()\n",
    "        elt.properties['speaker_external_org'] = elt.text_representation[location+2:location2+1].strip()\n",
    "        elt.properties['speaker_role'] = elt.text_representation[location2+4:].strip()\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    elif internal_speaker:\n",
    "        location = elt.text_representation.find('--')\n",
    "        elt.properties['speaker_name'] = elt.text_representation[:location].strip()\n",
    "        elt.properties['speaker_role'] = elt.text_representation[location+2:].strip()\n",
    "        elt.properties['speaker'] = True\n",
    "        elt.data[\"_break\"] = True\n",
    "    return elt\n",
    "\n",
    "# Also here's a nice way of writing chained pipelines\n",
    "merged_ds = (\n",
    "    extracted_ds\n",
    "    .map_elements(mark_speakers)\n",
    "    .merge(MarkedMerger())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf77c2d-140b-459c-86a0-40be0f5c39e8",
   "metadata": {},
   "source": [
    "## Initial Question Answering\n",
    "\n",
    "Now we should be able to answer our first question, even without a database behind it. With just a bunch of filters\n",
    "we are able to narrow down the docset to exectly the one document that answers:\n",
    "\n",
    "0. In the Broadcom earnings call, what details did the CFO, Kirsten Spears, discuss about the VMware acqusition?\n",
    "\n",
    "To answer this question we can do the following:\n",
    "\n",
    "1. Identify the Broadcom document\n",
    "2. Identify the elements where Kirsten Spears is speaking\n",
    "3. Identify the element where she mentions VMWare.\n",
    "\n",
    "We can translate this into a series of sycamore filters like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c070a3-724d-4245-937e-19bcb041d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcom_qads = (\n",
    "    merged_ds\n",
    "    .filter(lambda doc: doc.properties['entity']['company_ticker'] == 'AVGO')\n",
    "    .filter_elements(lambda elt: elt.properties.get('speaker_name') == 'Kirsten Spears')\n",
    "    .filter_elements(lambda elt: \"vmware\" in elt.text_representation.lower())\n",
    ")\n",
    "\n",
    "# Print the text of the first element that comes back\n",
    "print(broadcom_qads.take_all()[0].elements[0].text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670d135-327c-48b7-bd4c-992caff863e3",
   "metadata": {},
   "source": [
    "You'll notice that starting from the 5th paragraph Kirsten begins to discuss the VMWare acquisition and goes into great detail about how it impacted Broadcom's financials. \n",
    "\n",
    "Now let's try to answer another question on mongodb in a similar way.\n",
    "\n",
    "2. What did the MongoDB president mention about their competitor Amazon DynamoDB?\n",
    "\n",
    "We'll use a similar sort of plan:\n",
    "\n",
    "1. Filter to the mongodb document (stock ticker = \"MDB\")\n",
    "2. Filter to elements where the speaker role contains \"President\"\n",
    "3. Filter to an element containing 'DynamoDB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0293d5a-0268-4255-9922-2b1c146a7a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mongodb_qads = (\n",
    "    merged_ds\n",
    "    # Fill it out yo'self\n",
    ")\n",
    "\n",
    "\n",
    "print(mongodb_qads.take_all()[0].elements[0].text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e9e0f-7f7d-43ba-90af-5580a6bb71a1",
   "metadata": {},
   "source": [
    "I'll admit that this may look stupid. Why wouldn't we just write all the documents to a database and then do the search that way?\n",
    "Well, yes, we'll enable that next. But we'll come back to this docset-based strategy for question-answering, as it allows you to \n",
    "answer almost arbitrarily complex questions that a search database may not support.\n",
    "\n",
    "Now for the search enablement:\n",
    "\n",
    "## Embedding\n",
    "\n",
    "In order to do that, we'll need to write our docset to a database, and embed the text of our elements to use k-nearest-neighbor vector \n",
    "search to retrieve relevant chunks for an LLM to summarize.\n",
    "\n",
    "Embedding data with sycamore is fairly simple, so I'm going to give you all the information you need to do it and let you write it out.\n",
    "There is a method on DocSets called `embed()`. It takes an `Embedder` as its parameter. We'll use the `OpenAIEmbedder`, which you can\n",
    "import from `sycamore.transforms.embed`. It takes a `model_name` parameter which we'll set to `'text-embedding-3-small'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bffcfe-6a17-46f2-b55b-5515d88c9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from ... import ...\n",
    "\n",
    "embedded_ds = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d50874-867c-4256-aa16-22acac62c8eb",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "We'll be writing our data to Aryn (because what kind of workshop would this be if we didn't stand behind our own data warehouse). Sycamore can\n",
    "also write to a number of other systems, such as OpenSearch, ElasticSearch, Weaviate, etc. \n",
    "\n",
    "The unit of storage in Aryn equivalent to an index in OpenSearch or a table in a SQL DB is a 'DocSet.' While a Sycamore DocSet is usually best \n",
    "understood as a program, an Aryn DocSet is actually a container. We can create one using aryn_sdk, and then write our (sycamore) docset to it.\n",
    "\n",
    "First I'll add in a `spread_properties` transform, which copies properties from every Document to each of its Elements, so that all the elements have\n",
    "the `entity` and `path` metadata associated with the document. This will help my queries work properly; when filtering on a particular property,\n",
    "the elements all need to have that property in them, so I'll be able to filter by `entity.company_ticker = AVGO` and get the elements back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b60b36-498e-40d3-a6c0-89dab05203f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spread_ds = embedded_ds.spread_properties(['path', 'entity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290188d-da80-4642-9b41-056b7509e4fe",
   "metadata": {},
   "source": [
    "Now let's create our docset target (give it a name) and write to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3831d1-f191-4ad4-970e-91c557cb7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aryn_sdk.client.client import Client\n",
    "\n",
    "# You may need to specify aryn_api_key=\"<YOUR KEY>\" here\n",
    "aryn_client = Client()\n",
    "\n",
    "docset_name = \n",
    "aryn_docset = aryn_client.create_docset(name = docset_name)\n",
    "\n",
    "print(aryn_docset.value.docset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438ca2a-a544-41c9-8037-bdc34b74e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to specify aryn_api_key=\"<YOUR KEY>\" here too.\n",
    "spread_ds.write.aryn(aryn_url=\"https://api.aryn.ai/v1/storage\", docset_id=aryn_docset.value.docset_id, autoschema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332e92b-26d3-4542-8220-b8c9a2cfafff",
   "metadata": {},
   "source": [
    "Awesome! Now if you navigate to the [Aryn console](https://app.aryn.ai/storage) you should see your docset and the documents inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072d447e-908a-454a-b5cf-fe60a2dc5c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
